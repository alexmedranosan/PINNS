{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "979ba947-064b-4473-8f22-be54fa9e4489",
   "metadata": {},
   "source": [
    "## Problem setup\n",
    "\n",
    "In this Notebook we will solve the Euler classical equations using a PINN, that is, we will use a neural network in order to approach the numerical solution. We will write a straightforward code in Tensorflow from the beggining so all the Python code of this Notebook should be easy to export to a .py code.\n",
    "\n",
    "Thus, the classical Euler equations form a system of hyperbolic classical equations written explicitly as follows:\n",
    "\n",
    "$$\n",
    "(1)\\:\\:\\:\\partial_{t}\\rho+u\\partial_{x}\\rho+\\rho\\partial_{x}u=0\n",
    "$$\n",
    "\n",
    "$$\n",
    "(2)\\:\\:\\:\\rho\\left(\\partial_{t}u+u\\partial_{x}u\\right)+\\partial_{x}p=0\n",
    "$$\n",
    "\n",
    "$$\n",
    "(3)\\:\\:\\:\\partial_{t}p+\\gamma p\\partial_{x}u+\\partial_{x}p=0\n",
    "$$\n",
    "\n",
    "Where $\\rho$, $p$ and $u$ correspond to the density, the pressure and the velocity of the fluid, while $\\gamma=1.4$ is defined as the heat capacity ratio. This is, indeed, a Riemann problem, that is, a problem in first order partial derivative equations the initial conditions in space ($t=0$) present a discontinuity. These initial conditions are the following:\n",
    "\n",
    "$$\n",
    "\\text{Initial conditions ($t=0$)}=  \\left\\{\n",
    "\\begin{array}{ll}\n",
    "      \\rho_{0}:\\:\\:\\rho=1.0 & \\text{if $x\\leq 0.5$ and $\\rho=0.125$ else} \\\\\n",
    "      p_{0}:\\:\\:p=1.0 & \\text{if $x\\leq 0.5$ and $p=0.1$ else} \\\\\n",
    "      u_{0}:\\:\\:u=0 & \\forall x\\\\\n",
    "\\end{array} \n",
    "\\right.\n",
    "$$\n",
    "\n",
    "The method we will try builds an approximation for the solution using a neural network. Defining $\\textbf{U}=(\\rho,p,u)$ as the vector of variables, we have that the output of the PINN should be:\n",
    "\n",
    "$$\n",
    "\\textbf{U}_{\\theta}(t,x)\\approx\\textbf{U}(t,x),\n",
    "$$\n",
    "\n",
    "where $\\theta$ represents the internal parameters of the network. We have then $\\textbf{U}_{\\theta}(t,x):\\:[0,T]\\times\\Omega\\rightarrow\\mathbb{R}$, where $T$ is the final time and $\\Omega\\subset\\mathbb{R}^{d}$ represents the bounded spatial domain ($d$ corresponds to the number of spatial dimentions, equal to 1 in our case). We will also have boundary conditions which are applied in the boundary spatial domain, i.e. $\\partial\\Omega$ (the *tangent* space).\n",
    "\n",
    "Moving forward, the problem in neural networks can be seen as a problem of minimizing the loss function that can be seen as:\n",
    "\n",
    "$$\n",
    "L_{\\theta}(X)\\equiv L_{\\theta}^{r}(X^{r})+L_{\\theta}^{0}(X^{0})+L_{\\theta}^{b}(X^{b}),\n",
    "$$\n",
    "\n",
    "where $L_{\\theta}^{r}$ represents the residual loss, $L_{\\theta}^{0}$ the loss of the initial data and $L_{theta}^{b}$ is the loss related with the boundary conditions. Therefore, $X^{r}$, $X^{0}$ and $X^{b}$ correspond to the data of such spaces (space-time coordinates). In our case these losses will be applied all three as a mean squared error function.\n",
    "\n",
    "Finally, the boundary conditions will be of Dirichlet type, which will take the values of the initial condition. Therefore, we can ignore the associated loss in the total summation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0048f0-1e45-442a-92f2-366169f1b4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import TensorFlow and NumPy\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import sys\n",
    "sys.path.append(\"../../../../../\")\n",
    "import utils\n",
    "import math\n",
    "\n",
    "print(\"Is Tensorflow using GPU? \", tf.test.is_gpu_available())\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "os.environ[\"TF_GPU_ALLOCATOR\"]=\"cuda_malloc_async\"\n",
    "\n",
    "# Set data type:\n",
    "DTYPE = \"float32\"\n",
    "tf.keras.backend.set_floatx(DTYPE)\n",
    "\n",
    "# Set constants\n",
    "gamma = 1.4\n",
    "eps = 0.00001\n",
    "\n",
    "# Let's first define the initial conditions, that is, U_0:\n",
    "def fun_U_0(x):\n",
    "    x_numpy = x.numpy()\n",
    "    ic_rho = lambda x: 1.0 * (x <= 0.5) + 0.125 * (x > 0.5)\n",
    "    ic_u = lambda x: 0. * (x <= 0.5) + 0. *(x > 0.5)\n",
    "    ic_p = lambda x: 1.0 * (x <= 0.5) + 0.1 * (x > 0.5)\n",
    "    \n",
    "    return [tf.constant(ic_rho(x_numpy), dtype = DTYPE), tf.constant(ic_u(x_numpy), dtype = DTYPE), tf.constant(ic_p(x_numpy), dtype = DTYPE)]\n",
    "\n",
    "# Define residual of the PDE system:\n",
    "def fun_r(t, x, rho, p, u, rho_t, p_t, u_t, rho_x, p_x, u_x):\n",
    "    loss_r = (rho_t + u * rho_x + rho * u_x) ** 2 + (rho * (u_t + u * u_x) + p_x) ** 2 + (p_t + gamma * p * u_x + u * p_x) ** 2\n",
    "    return loss_r\n",
    "\n",
    "weight_IC_initial, weight_RES_initial = 10.0, 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc86499-c281-48fa-8bc9-20ce1de5b462",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set number of data points\n",
    "N_0 = 5000\n",
    "N_b = 5000\n",
    "N_r = 30000\n",
    "\n",
    "# Set boundary\n",
    "tmin, tmax, xmin, xmax = 0., 0.2, 0., 1.0\n",
    "\n",
    "# Lower bounds and Upper bounds.\n",
    "lb, ub = tf.constant([tmin, xmin], dtype = DTYPE), tf.constant([tmax, xmax], dtype = DTYPE)\n",
    "# Set random seed for reproducible results.\n",
    "tf.random.set_seed(0)\n",
    "\n",
    "# Draw uniform sample points for initial boundary data.\n",
    "t_0 = tf.ones((N_0,1), dtype = DTYPE)*lb[0]\n",
    "x_0 = tf.random.uniform((N_0,1), lb[1], ub[1], dtype = DTYPE)\n",
    "X_0 = tf.concat([t_0, x_0], axis = 1)\n",
    "# Evaluate intitial condition at x_0. U_0 will be a list of 3 items, each one of them the initial values for each variable (rho, p, u):\n",
    "U_0 = fun_U_0(x_0)\n",
    "\n",
    "# Draw uniformly sampled collocation points (internal points for the residual part):\n",
    "t_r = tf.random.uniform((N_r,1), lb[0], ub[0], dtype = DTYPE)\n",
    "x_r = tf.random.uniform((N_r,1), lb[1], ub[1], dtype = DTYPE)\n",
    "X_r = tf.concat([t_r, x_r], axis = 1)\n",
    "\n",
    "# Collect boundary and inital data in lists:\n",
    "X_data = X_0 #[X_0, X_b]\n",
    "U_data = U_0 #[U_0, U_b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799c9e80-4d6f-461a-a9f6-3566da536ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure(figsize=(9,6))\n",
    "plt.scatter(t_0, x_0, c = \"g\", marker = \"X\", vmin = -1, vmax = 1, label = \"Initial data\")\n",
    "#plt.scatter(t_b, x_b, c = \"b\", marker = \"X\", vmin = -1, vmax = 1, label = \"Boundary data\")\n",
    "plt.scatter(t_r, x_r, c = \"r\", marker = \".\", alpha = 0.1, label = \"Internal data\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"t\")\n",
    "plt.ylabel(\"x\")\n",
    "\n",
    "plt.title(\"Positions of training points\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51012f9-663b-491f-a83e-3c6f243cb047",
   "metadata": {},
   "source": [
    "### Set up network architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7857556a-7bc3-43df-a490-9fb0cb229cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a Model class:\n",
    "class PINN(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.dense1 = tf.keras.layers.Dense(30, activation = tf.nn.tanh)\n",
    "        self.dense2 = tf.keras.layers.Dense(30, activation = tf.nn.tanh)\n",
    "        self.dense3 = tf.keras.layers.Dense(30, activation = tf.nn.tanh)\n",
    "        self.dense4 = tf.keras.layers.Dense(30, activation = tf.nn.tanh)\n",
    "        self.dense5 = tf.keras.layers.Dense(30, activation = tf.nn.tanh)\n",
    "        self.dense6 = tf.keras.layers.Dense(30, activation = tf.nn.tanh)\n",
    "        self.dense7 = tf.keras.layers.Dense(30, activation = tf.nn.tanh)\n",
    "        self.dense8 = tf.keras.layers.Dense(30, activation = tf.nn.tanh)\n",
    "        self.dense_output = tf.keras.layers.Dense(3, activation = tf.nn.tanh)\n",
    "    def call(self, inputs):\n",
    "        y = self.dense1(inputs)\n",
    "        y = self.dense2(y)\n",
    "        y = self.dense3(y)\n",
    "        y = self.dense4(y)\n",
    "        y = self.dense5(y)\n",
    "        y = self.dense6(y)\n",
    "        y = self.dense7(y)\n",
    "        y = self.dense8(y)\n",
    "        \n",
    "        final_output = self.dense_output(y)\n",
    "        \n",
    "        return final_output\n",
    "        \n",
    "model = PINN()\n",
    "model.build(input_shape = X_r.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4796dec8-6001-4c6c-af07-ccb367f70892",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, in order to define the losses, we need to compute the needed partial derivatives using the automatic differentiation of Tensorflow.\n",
    "# We will define a variable that will give us the residual loss.\n",
    "\n",
    "def get_r(model, X_r):\n",
    "    \n",
    "    # A tf.GradientTape is used to compute derivatives in TensorFlow\n",
    "    with tf.GradientTape(persistent = True) as tape:\n",
    "        # Split t and x to compute partial derivatives\n",
    "        t, x = X_r[:, 0:1], X_r[:,1:2]\n",
    "\n",
    "        # Variables t and x are watched during tape to compute the partial derivatives\n",
    "        tape.watch(t)\n",
    "        tape.watch(x)\n",
    "\n",
    "        # Determine residual \n",
    "        U = model(tf.stack([t[:,0], x[:,0]], axis = 1))\n",
    "        rho, u, p = U[:,0], U[:,1], U[:,2]\n",
    "        rho, u, p = tf.reshape(rho, shape = (rho.shape[0],1)), tf.reshape(u, shape = (u.shape[0],1)), tf.reshape(p, shape = (p.shape[0],1))\n",
    "        \n",
    "    rho_t = tape.gradient(rho, t)\n",
    "    u_t = tape.gradient(u, t)\n",
    "    p_t = tape.gradient(p, t)\n",
    "\n",
    "    rho_x = tape.gradient(rho, x)\n",
    "    u_x = tape.gradient(u, x)\n",
    "    p_x = tape.gradient(p, x)\n",
    "\n",
    "    del tape\n",
    "\n",
    "    return fun_r(t, x, rho, p, u, rho_t, p_t, u_t, rho_x, p_x, u_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3eff73b-f3cd-4073-9e31-f91a7950f7e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss_res(model, X_r, X_data, U_data):\n",
    "    # The \"real\" data corresponds to U_data.\n",
    "    rho_ic, u_ic, p_ic = U_data[0], U_data[1], U_data[2]\n",
    "    \n",
    "    # Compute L_r.\n",
    "    r = get_r(model, X_r)\n",
    "    loss_r = tf.reduce_mean(r)\n",
    "    \n",
    "    return loss_r\n",
    "\n",
    "def compute_loss_IC(model, X_r, X_data, U_data):\n",
    "    # The \"real\" data corresponds to U_data.\n",
    "    rho_ic, u_ic, p_ic = U_data[0], U_data[1], U_data[2]\n",
    "    \n",
    "    # Prediction:\n",
    "    U_pred_initial = model(X_data)\n",
    "    \n",
    "    rho_nn_ic, u_nn_ic, p_nn_ic = U_pred_initial[:,0], U_pred_initial[:,1], U_pred_initial[:,2]\n",
    "    rho_nn_ic_reshaped, u_nn_ic_reshaped, p_nn_ic_reshaped = tf.reshape(rho_nn_ic, shape = rho_ic.shape), tf.reshape(u_nn_ic, shape = u_ic.shape), tf.reshape(p_nn_ic, shape = p_ic.shape)\n",
    "\n",
    "    loss_ic = tf.reduce_mean((u_nn_ic_reshaped - u_ic) ** 2 + (rho_nn_ic_reshaped - rho_ic) ** 2 + (p_nn_ic_reshaped - p_ic) ** 2)\n",
    "    \n",
    "    return loss_ic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57853bdd-148a-4564-8219-0c6e89b309f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We now can set up the optimizer and train the model.\n",
    "# Initialize model aka U_theta\n",
    "#model = init_model()\n",
    "\n",
    "lr = 1e-5\n",
    "# It would be also posible to define a piecewise learning rate.\n",
    "#lr = tf.keras.optimizers.schedules.PiecewiseConstantDecay([1000,3000],[1e-2,1e-3,5e-4])\n",
    "\n",
    "# Choose the optimizer\n",
    "optim = tf.keras.optimizers.Adam(learning_rate = lr)\n",
    "#optim = tf.keras.optimizers.Adadelta(learning_rate = lr)\n",
    "#optim = tf.keras.optimizers.RMSprop(learning_rate = lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93aff9fc-0f72-4e5e-b2c1-bcf5d1f1293d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, dict_data = utils.load_model_data(path_model = \"Models_Data/Euler_raw_variables_model_varying_weights\", path_data = \"Models_Data/Euler_raw_variables_model_varying_weights.h5\")\n",
    "X_r = tf.constant(dict_data[\"X_r\"], dtype = DTYPE)\n",
    "X_0 = tf.constant(dict_data[\"X_0\"], dtype = DTYPE)\n",
    "loss_ic_hist = list(dict_data[\"loss_ic_hist\"])\n",
    "loss_r_hist = list(dict_data[\"loss_r_hist\"])\n",
    "loss_hist = list(np.array(loss_ic_hist) + np.array(loss_r_hist))\n",
    "weights_RES = list(dict_data[\"weights_RES\"])\n",
    "weights_IC = list(dict_data[\"weights_IC\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339d74ef-aa4f-4217-ae1b-c489335318fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2000\n",
    "\n",
    "# Make batches from the data.\n",
    "\n",
    "X_r_batches = tf.data.Dataset.from_tensor_slices(X_r.numpy()).batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262c89f5-7aae-49f4-9376-fb25fd8682b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(loss_hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dffc2b96-1744-4cd2-88c6-638f928ec12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define one training step as a TensorFlow function to increase speed of training\n",
    "# Number of training epochs\n",
    "epochs = 100000\n",
    "alpha = 1e-4\n",
    "#loss_hist, loss_ic_hist, loss_r_hist = [], [], []\n",
    "#weights_RES, weights_IC = [], []\n",
    "\n",
    "for epoch in range(len(loss_hist), epochs):\n",
    "    if epoch == 0:\n",
    "        for step, X_r_batch in enumerate(X_r_batches):\n",
    "            with tf.GradientTape(persistent=True) as tape:\n",
    "                # This tape is for derivatives with respect to trainable variables\n",
    "                tape.watch(model.trainable_variables)\n",
    "                loss_r = compute_loss_res(model, X_r_batch, X_data, U_data)\n",
    "                loss_ic = compute_loss_IC(model, X_r_batch, X_data, U_data)\n",
    "                loss = weight_RES_initial * loss_r + weight_IC_initial * loss_ic\n",
    "\n",
    "            grad_theta = tape.gradient(loss, model.trainable_variables)\n",
    "            del tape\n",
    "            \n",
    "            optim.apply_gradients(zip(grad_theta, model.trainable_variables))\n",
    "        weights_RES.append(weight_RES_initial)\n",
    "        weights_IC.append(weight_IC_initial)\n",
    "    else:\n",
    "        for step, X_r_batch in enumerate(X_r_batches):\n",
    "            with tf.GradientTape(persistent=True) as tape:\n",
    "                # This tape is for derivatives with respect to trainable variables\n",
    "                tape.watch(model.trainable_variables)\n",
    "                loss_r = compute_loss_res(model, X_r_batch, X_data, U_data)\n",
    "                loss_ic = compute_loss_IC(model, X_r_batch, X_data, U_data)\n",
    "                \n",
    "                weight_RES = weights_RES[len(weights_RES) - 1] * math.exp(alpha * loss_r / (loss_r + loss_ic))\n",
    "                weight_IC = weights_IC[len(weights_IC) - 1] * math.exp(-alpha * loss_ic / (loss_r + loss_ic))\n",
    "                \n",
    "                loss = weight_RES * loss_r + weight_IC * loss_ic\n",
    "\n",
    "            grad_theta = tape.gradient(loss, model.trainable_variables)\n",
    "            del tape\n",
    "            \n",
    "            optim.apply_gradients(zip(grad_theta, model.trainable_variables))\n",
    "        weights_RES.append(weight_RES)\n",
    "        weights_IC.append(weight_IC)\n",
    "\n",
    "    loss_hist.append(loss.numpy())\n",
    "    loss_ic_hist.append(loss_ic.numpy())\n",
    "    loss_r_hist.append(loss_r.numpy())\n",
    "    if epoch%10 == 0:\n",
    "        print(\"Epoch \", str(epoch), \": Total Loss = \", str(float(loss)), \", Loss IC = \", str(float(loss_ic)), \", Loss R = \", str(float(loss_r)))\n",
    "    #if epoch%5000 == 0:\n",
    "    #    X_r_batches = add_extra_points(model, X_r, new_points = 50, batch_size = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "436fb085-1251-4ad7-a334-46031fa4ac40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot of weights\n",
    "fig = plt.figure(figsize = (15,6))\n",
    "ax1 = fig.add_subplot(1, 2, 1)\n",
    "ax2 = fig.add_subplot(1, 2, 2)\n",
    "ax1.plot(range(0, len(weights_RES)), weights_RES, \"k-\")\n",
    "ax1.set_title(\"Residual weights\")\n",
    "ax2.plot(range(0, len(weights_IC)), weights_IC, \"b-\")\n",
    "ax2.set_title(\"IC weights\")\n",
    "plt.savefig(\"Images/Adaptative_Weights_evolution.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93739010-dff8-4476-9b3b-bfb78df38215",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot of the losses\n",
    "fig = plt.figure(figsize=(9,6))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.semilogy(range(0, 3000), loss_hist[:3000], \"k-\", label = \"Total loss\")\n",
    "#ax.semilogy(range(0, 3000), 10*np.array(loss_ic_hist[:3000]), \"b-\", label = \"IC loss\")\n",
    "#ax.semilogy(range(0, 3000), 0.1*np.array(loss_r_hist[:3000]), \"r-\", label = \"Res. loss\")\n",
    "ax.set_xlabel('$n_{epoch}$')\n",
    "ax.set_ylabel('$\\\\L_{n_{epoch}}$')\n",
    "plt.legend()\n",
    "plt.savefig(\"Images/Training_Losses_Euler_raw_variables.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30889dd0-589b-436f-808a-491c5ac79a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model and data\n",
    "import h5py\n",
    "\n",
    "model.save(\"Models_Data/Euler_raw_variables_model_varying_weights\")\n",
    "\n",
    "hf = h5py.File(\"Models_Data/Euler_raw_variables_model_varying_weights.h5\", \"w\")\n",
    "hf.create_dataset(\"X_r\", data = X_r.numpy())\n",
    "hf.create_dataset(\"X_0\", data = X_data.numpy())\n",
    "hf.create_dataset(\"rho_ic\", data = U_data[0].numpy())\n",
    "hf.create_dataset(\"u_ic\", data = U_data[1].numpy())\n",
    "hf.create_dataset(\"p_ic\", data = U_data[2].numpy())\n",
    "hf.create_dataset(\"loss_r_hist\", data = np.array(loss_r_hist))\n",
    "hf.create_dataset(\"loss_ic_hist\", data = np.array(loss_ic_hist))\n",
    "hf.create_dataset(\"weights_RES\", data = np.array(weights_RES))\n",
    "hf.create_dataset(\"weights_IC\", data = np.array(weights_IC))\n",
    "hf.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
